The idea of paper https://arxiv.org/pdf/2509.06160 is to use reversed engineering to generate the reasoning process Z based on user query X and reference answer Y. 
The original idea of this process is to distill the thinking capabilities from stronger models like o1/r1, and use these distilled thinking process to train smaller models to do similar thinking. However, the question here is how could we measure the quality of distilled thinking process and how could we optimize the thinking process. This paper's idea is to use Perplexity as a proxy to measure the quality (lower PPL means the model are more confident; while higher PPL means the model is more supervise with the content). And they propose a local search algorithm to optimize this thinking process Z.
The paper use the method to curated a set of datasets and use them to train a 8B model. They showcase that the trained model has a good performance on some open-ended tasks.
The problem it want to solve is a general problem. Let's consider the situation of agentic learning. The long-horizon agentic tasks require the agents to interact with the envs for multiple turns. Each interaction work like: the agent do some reasoning first, then propose the actions, the actions then are parsed to call the tools to interact with the env, the env then return the agents with the tool invocation results, the agents then based on the new results to do next round interaction.
So in this process, the agents' reasoning is based on several parts: (1) the user query; (2) the memory context; (3) the tools invocation results;
If we apply the similar idea of reversed engineered reasoning, the process is like we provide the agent with the user query X, and the reference answer Y, how to get the entire agent's reasoning trajectory?
But this a multi-turn process, we cannot directly leverage the reversed-engineered techniques to this problem. We need to adjust.
And also it seems PPL might not be suitable, as the reasoning process also have to consider the tools invocation results (which usually should be ood for the agents), the reasoning on these results may always be supervise to the model. We might need new measurement.

**The core question here is how to evolve from single turn to multi-turn (long horizon). What change?**
Can long horizon agentic tasks still be benefited from strong LLM models (which are usually single-turn trained policy model)?